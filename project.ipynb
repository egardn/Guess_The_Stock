{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10853866,"sourceType":"datasetVersion","datasetId":6741501}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nX_TRAIN_PICKLE_PATH = '/kaggle/input/cfm-gts/X_train.pkl'\nY_TRAIN_PICKLE_PATH = '/kaggle/input/cfm-gts/y_train.pkl'","metadata":{"id":"BSBQgGvYJ7G0","outputId":"f3847469-908f-4a27-df32-1cbeeab756a2","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T18:47:31.028533Z","iopub.execute_input":"2025-02-25T18:47:31.028898Z","iopub.status.idle":"2025-02-25T18:47:31.033718Z","shell.execute_reply.started":"2025-02-25T18:47:31.028865Z","shell.execute_reply":"2025-02-25T18:47:31.032785Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, Input, Dense, GRU, Concatenate, Flatten, Reshape, TimeDistributed\nfrom tensorflow.keras.models import Model\n\n\n# ----------------------------------\n# 1️⃣ Load & Cache Data Efficiently\n# ----------------------------------\n\nif os.path.exists(X_TRAIN_PICKLE_PATH) and os.path.exists(Y_TRAIN_PICKLE_PATH):\n    X_train = pd.read_pickle(X_TRAIN_PICKLE_PATH)\n    y_train = pd.read_pickle(Y_TRAIN_PICKLE_PATH)","metadata":{"id":"s2T_qbsnHQ_w","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T18:47:31.034698Z","iopub.execute_input":"2025-02-25T18:47:31.035049Z","iopub.status.idle":"2025-02-25T18:47:36.334086Z","shell.execute_reply.started":"2025-02-25T18:47:31.035015Z","shell.execute_reply":"2025-02-25T18:47:36.333277Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T18:47:36.335001Z","iopub.execute_input":"2025-02-25T18:47:36.335353Z","iopub.status.idle":"2025-02-25T18:47:36.352537Z","shell.execute_reply.started":"2025-02-25T18:47:36.335318Z","shell.execute_reply":"2025-02-25T18:47:36.351475Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   obs_id  venue  order_id action side  price  bid   ask  bid_size  ask_size  \\\n0       0      4         0      A    A   0.30  0.0  0.01       100         1   \n1       0      4         1      A    B  -0.17  0.0  0.01       100         1   \n2       0      4         2      D    A   0.28  0.0  0.01       100         1   \n3       0      4         3      A    A   0.30  0.0  0.01       100         1   \n4       0      4         4      D    A   0.37  0.0  0.01       100         1   \n\n   trade  flux  \n0  False   100  \n1  False   100  \n2  False  -100  \n3  False   100  \n4  False  -100  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_id</th>\n      <th>venue</th>\n      <th>order_id</th>\n      <th>action</th>\n      <th>side</th>\n      <th>price</th>\n      <th>bid</th>\n      <th>ask</th>\n      <th>bid_size</th>\n      <th>ask_size</th>\n      <th>trade</th>\n      <th>flux</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>A</td>\n      <td>A</td>\n      <td>0.30</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>False</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>B</td>\n      <td>-0.17</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>False</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>D</td>\n      <td>A</td>\n      <td>0.28</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>False</td>\n      <td>-100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>A</td>\n      <td>A</td>\n      <td>0.30</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>False</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>D</td>\n      <td>A</td>\n      <td>0.37</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n      <td>False</td>\n      <td>-100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, GRU, Dense, Concatenate, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport random\nfrom sklearn.model_selection import train_test_split\n\n# Constants for the embedding dimensions\nVENUE_EMBED_DIM = 8\nACTION_EMBED_DIM = 8\nTRADE_EMBED_DIM = 8\nSEQ_LENGTH = 100  # Define sequence length as a global constant\n\nclass OrderBookSequenceReshaper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Reshapes order book data into sequences of observations by obs_id.\n    Input: DataFrame with order book events\n    Output: Dictionary of obs_id -> sequence of 100 events\n    \"\"\"\n    def __init__(self, seq_length=SEQ_LENGTH):\n        self.seq_length = seq_length\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Group data by observation ID\n        grouped = X.groupby('obs_id')\n        sequences = {}\n\n        for obs_id, group in grouped:\n            # Ensure we have exactly seq_length observations\n            if len(group) == self.seq_length:\n                sequences[obs_id] = group.reset_index(drop=True)\n            else:\n                print(f\"Warning: Observation {obs_id} has {len(group)} events, expected {self.seq_length}\")\n\n        return sequences\n\nclass FeatureVectorizer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms sequences of order book events into tensors of shape (100, 30)\n    \"\"\"\n    def __init__(self, seq_length=SEQ_LENGTH):\n        self.venue_mapping = None\n        self.action_mapping = None\n        self.seq_length = seq_length\n\n    def fit(self, sequences, y=None):\n        # Extract unique values for categorical features\n        all_venues = set()\n        all_actions = set()\n\n        for seq in sequences.values():\n            all_venues.update(seq['venue'].unique())\n            all_actions.update(seq['action'].unique())\n\n        # Create mappings for categorical features\n        self.venue_mapping = {v: i for i, v in enumerate(sorted(all_venues))}\n        self.action_mapping = {a: i for i, a in enumerate(sorted(all_actions))}\n\n        return self\n\n    def transform(self, sequences):\n        result = []\n        obs_ids = []\n\n        for obs_id, seq in sequences.items():\n            # Get first bid price to normalize prices\n            first_bid = seq.iloc[0]['bid']\n\n            # Initialize tensor for this sequence\n            seq_tensor = np.zeros((self.seq_length, 30))\n\n            for i, row in seq.iterrows():\n                # 1-8: Venue embedding (one-hot for now, will be replaced by embedding)\n                venue_idx = self.venue_mapping[row['venue']]\n                seq_tensor[i, venue_idx % 8] = 1  # Use modulo to fit within 8 dimensions\n\n                # 9-16: Action embedding\n                action_idx = self.action_mapping[row['action']]\n                seq_tensor[i, 8 + (action_idx % 8)] = 1  # Use modulo to fit within 8 dimensions\n\n                # 17-24: Trade embedding (one-hot for boolean)\n                trade_val = 1 if row['trade'] else 0\n                seq_tensor[i, 16 + trade_val] = 1\n\n                # 25: Normalized bid\n                seq_tensor[i, 24] = row['bid'] - first_bid\n\n                # 26: Normalized ask\n                seq_tensor[i, 25] = row['ask'] - first_bid\n\n                # 27: Normalized price\n                seq_tensor[i, 26] = row['price'] - first_bid\n\n                # 28: log(bid_size + 1)\n                seq_tensor[i, 27] = np.log1p(row['bid_size'])\n\n                # 29: log(ask_size + 1)\n                seq_tensor[i, 28] = np.log1p(row['ask_size'])\n\n                # 30: log(flux)\n                # Handle flux which could be negative\n                flux = row['flux']\n                seq_tensor[i, 29] = np.sign(flux) * np.log1p(abs(flux))\n\n            result.append(seq_tensor)\n            obs_ids.append(obs_id)\n\n        return np.array(result), obs_ids\n\nclass OrderBookEmbeddingModel(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Neural network model for order book classification with proper embeddings\n    \"\"\"\n    def __init__(self, n_venues, n_actions, n_categories=24, batch_size=128, learning_rate=3e-3, n_batches=10000):\n        self.n_venues = n_venues\n        self.n_actions = n_actions\n        self.n_categories = n_categories\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.n_batches = n_batches\n        self.model = None\n\n    def create_model(self):\n        # Input: (100, 30)\n        input_layer = Input(shape=(SEQ_LENGTH, 30))\n\n        # GRU layers - forward and backward\n        forward_gru = GRU(64, return_sequences=False)(input_layer)\n        backward_gru = GRU(64, return_sequences=False, go_backwards=True)(input_layer)\n\n        # Concatenate GRU outputs\n        concat = Concatenate()([forward_gru, backward_gru])\n\n        # Dense layers\n        dense1 = Dense(64, activation='selu')(concat)\n        output_layer = Dense(self.n_categories, activation='softmax')(dense1)\n\n        # Create and compile model\n        model = Model(inputs=input_layer, outputs=output_layer)\n        model.compile(\n            loss='sparse_categorical_crossentropy',\n            optimizer=Adam(learning_rate=self.learning_rate),\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def fit(self, X, y):\n        # X should be the tensor array of shape (n_samples, 100, 30)\n        # y should be the category labels\n\n        self.model = self.create_model()\n\n        # Print model summary\n        self.model.summary()\n\n        # Training loop for specified number of batches\n        for batch in range(self.n_batches):\n            # Randomly select batch_size samples\n            if len(X) <= self.batch_size:\n                X_batch = X\n                y_batch = y\n            else:\n                indices = random.sample(range(len(X)), self.batch_size)\n                X_batch = X[indices]\n                y_batch = y[indices]\n\n            # Train on batch\n            loss, acc = self.model.train_on_batch(X_batch, y_batch)\n\n            # Report progress every 10 batches\n            if batch % 10 == 0:\n                print(f\"Batch {batch}/{self.n_batches} completed - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n\n        return self\n\n    def transform(self, X):\n        # Return predictions\n        return self.model.predict(X)\n\n    def predict(self, X):\n        # Return class predictions\n        return np.argmax(self.model.predict(X), axis=1)\n\n# Define the full pipeline\ndef create_order_book_pipeline():\n    pipeline = Pipeline([\n        ('reshaper', OrderBookSequenceReshaper()),\n        ('vectorizer', FeatureVectorizer()),\n        # The model would be added after preprocessing in the training script\n    ])\n\n    return pipeline\n\n# Training script with subset selection\ndef train_order_book_model(X_train, y_train, n_observations=200, n_batches=100):\n    \"\"\"\n    Train the model on a subset of data\n\n    Parameters:\n    - X_train: DataFrame with order book events\n    - y_train: DataFrame with labels\n    - n_observations: Number of observations to use for training\n    - n_batches: Number of batches to train for\n    \"\"\"\n    print(f\"Selecting {n_observations} random observations for training...\")\n\n    # Get unique observation IDs\n    obs_ids = X_train['obs_id'].unique()\n\n    # Select a random subset of observation IDs\n    if len(obs_ids) > n_observations:\n        selected_obs_ids = np.random.choice(obs_ids, size=n_observations, replace=False)\n    else:\n        selected_obs_ids = obs_ids\n        print(f\"Warning: Only {len(obs_ids)} observations available\")\n\n    # Filter data to only include selected observations\n    X_train_subset = X_train[X_train['obs_id'].isin(selected_obs_ids)]\n    y_train_subset = y_train[y_train.iloc[:, 0].isin(selected_obs_ids)]\n\n    print(f\"Selected data shape: {X_train_subset.shape}\")\n\n    # Step 1: Data preprocessing\n    pipeline = create_order_book_pipeline()\n    sequences = pipeline.named_steps['reshaper'].fit_transform(X_train_subset)\n    print(f\"Generated {len(sequences)} valid sequences\")\n\n    X_tensors, obs_ids = pipeline.named_steps['vectorizer'].fit_transform(sequences)\n    print(f\"Tensor shape: {X_tensors.shape}\")\n\n    # Step 2: Map observation IDs to labels\n    y_dict = dict(zip(y_train.iloc[:, 0], y_train.iloc[:, 1]))\n    y_values = np.array([y_dict[obs_id] for obs_id in obs_ids])\n    print(f\"Label shape: {y_values.shape}\")\n\n    # Step 3: Create and train the model\n    n_venues = len(pipeline.named_steps['vectorizer'].venue_mapping)\n    n_actions = len(pipeline.named_steps['vectorizer'].action_mapping)\n\n    print(f\"Unique venues: {n_venues}, Unique actions: {n_actions}\")\n\n    model = OrderBookEmbeddingModel(\n        n_venues=n_venues,\n        n_actions=n_actions,\n        n_batches=n_batches\n    )\n\n    print(f\"Starting training for {n_batches} batches...\")\n    model.fit(X_tensors, y_values)\n\n    return model, pipeline\n\n# Main function with small subset training\ndef main(n_observations=500, n_batches=100, test_size=0.2):\n    \"\"\"\n    Run the pipeline on a small subset of data with proper train/val splitting\n    \"\"\"\n    print(\"Starting training pipeline with small subset...\")\n\n    # Get all unique observation IDs first\n    all_obs_ids = X_train['obs_id'].unique()\n\n    # Limit to n_observations if specified\n    if len(all_obs_ids) > n_observations:\n        all_selected_ids = np.random.choice(all_obs_ids, size=n_observations, replace=False)\n    else:\n        all_selected_ids = all_obs_ids\n\n    # Split IDs into train and validation BEFORE any processing\n    train_ids, val_ids = train_test_split(all_selected_ids, test_size=test_size, random_state=42)\n\n    print(f\"Selected {len(train_ids)} observations for training and {len(val_ids)} for validation\")\n\n    # Filter data for training\n    X_train_subset = X_train[X_train['obs_id'].isin(train_ids)]\n    y_train_subset = y_train[y_train.iloc[:, 0].isin(train_ids)]\n\n    # Create pipeline for preprocessing\n    pipeline = create_order_book_pipeline()\n\n    # Process training data\n    train_sequences = pipeline.named_steps['reshaper'].fit_transform(X_train_subset)\n    print(f\"Generated {len(train_sequences)} valid sequences for training from {len(train_ids)} observations\")\n\n    # If no valid sequences, return early\n    if len(train_sequences) == 0:\n        print(\"No valid training sequences found. Check your data.\")\n        return None, pipeline\n\n    X_train_tensors, train_obs_ids = pipeline.named_steps['vectorizer'].fit_transform(train_sequences)\n\n    # Map observation IDs to labels\n    y_dict = dict(zip(y_train.iloc[:, 0], y_train.iloc[:, 1]))\n    y_train_values = np.array([y_dict[obs_id] for obs_id in train_obs_ids])\n\n    # Create and train model\n    n_venues = len(pipeline.named_steps['vectorizer'].venue_mapping)\n    n_actions = len(pipeline.named_steps['vectorizer'].action_mapping)\n\n    print(f\"Unique venues: {n_venues}, Unique actions: {n_actions}\")\n    print(f\"Training tensor shape: {X_train_tensors.shape}\")\n    print(f\"Training labels shape: {y_train_values.shape}\")\n\n    model = OrderBookEmbeddingModel(\n        n_venues=n_venues,\n        n_actions=n_actions,\n        n_batches=n_batches\n    )\n\n    print(f\"Starting training for {n_batches} batches...\")\n    model.fit(X_train_tensors, y_train_values)\n\n    # Process validation data if available\n    if len(val_ids) > 0:\n        print(f\"\\nValidating on {len(val_ids)} observations...\")\n\n        X_val_subset = X_train[X_train['obs_id'].isin(val_ids)]\n\n        # Process validation data independently\n        val_sequences = pipeline.named_steps['reshaper'].transform(X_val_subset)\n        print(f\"Generated {len(val_sequences)} valid sequences for validation from {len(val_ids)} observations\")\n\n        if len(val_sequences) == 0:\n            print(\"No valid validation sequences found. Skipping validation.\")\n            return model, pipeline\n\n        X_val_tensors, val_obs_ids = pipeline.named_steps['vectorizer'].transform(val_sequences)\n\n        # Map observation IDs to labels\n        y_val_values = np.array([y_dict[obs_id] for obs_id in val_obs_ids])\n\n        # Make predictions\n        print(f\"Making predictions on {len(val_obs_ids)} validation samples...\")\n        val_predictions = model.predict(X_val_tensors)\n\n        # Calculate accuracy\n        accuracy = (val_predictions == y_val_values).mean()\n        print(f\"Validation accuracy: {accuracy:.4f}\")\n\n    return model, pipeline","metadata":{"id":"6rUiRg2nHQ_0","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T18:47:36.353763Z","iopub.execute_input":"2025-02-25T18:47:36.354085Z","iopub.status.idle":"2025-02-25T18:47:36.405649Z","shell.execute_reply.started":"2025-02-25T18:47:36.354060Z","shell.execute_reply":"2025-02-25T18:47:36.404743Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model, pipeline = main(n_observations=1000, n_batches=100)\n\nif model and pipeline:\n        with open('order_book_model.pkl', 'wb') as f:\n            pickle.dump(model, f)\n        with open('order_book_pipeline.pkl', 'wb') as f:\n            pickle.dump(pipeline, f)","metadata":{"id":"EPSFSgC2Vy-J","outputId":"e7202c4b-2dad-4127-97c8-80014f26f46d","colab":{"base_uri":"https://localhost:8080/","height":720},"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T18:47:36.407774Z","iopub.execute_input":"2025-02-25T18:47:36.408022Z","iopub.status.idle":"2025-02-25T18:48:27.261262Z","shell.execute_reply.started":"2025-02-25T18:47:36.408002Z","shell.execute_reply":"2025-02-25T18:48:27.260376Z"}},"outputs":[{"name":"stdout","text":"Starting training pipeline with small subset...\nSelected 800 observations for training and 200 for validation\nGenerated 800 valid sequences for training from 800 observations\nUnique venues: 6, Unique actions: 3\nTraining tensor shape: (800, 100, 30)\nTraining labels shape: (800,)\nStarting training for 100 batches...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ gru (\u001b[38;5;33mGRU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m18,432\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m18,432\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │          \u001b[38;5;34m1,560\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,432</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,432</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,560</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,680\u001b[0m (182.34 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,680</span> (182.34 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,680\u001b[0m (182.34 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,680</span> (182.34 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Batch 0/100 completed - Loss: 3.7044, Accuracy: 0.0234\nBatch 10/100 completed - Loss: 3.3034, Accuracy: 0.0597\nBatch 20/100 completed - Loss: 3.1776, Accuracy: 0.0785\nBatch 30/100 completed - Loss: 3.1058, Accuracy: 0.0950\nBatch 40/100 completed - Loss: 3.0350, Accuracy: 0.1183\nBatch 50/100 completed - Loss: 2.9643, Accuracy: 0.1368\nBatch 60/100 completed - Loss: 2.8901, Accuracy: 0.1507\nBatch 70/100 completed - Loss: 2.8243, Accuracy: 0.1655\nBatch 80/100 completed - Loss: 2.7608, Accuracy: 0.1810\nBatch 90/100 completed - Loss: 2.6986, Accuracy: 0.1963\n\nValidating on 200 observations...\nGenerated 200 valid sequences for validation from 200 observations\nMaking predictions on 200 validation samples...\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\nValidation accuracy: 0.1500\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install fastapi uvicorn nest-asyncio pyngrok\n\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Union, Optional\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport os\nfrom datetime import datetime\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# FastAPI application\napp = FastAPI(\n    title=\"Order Book Prediction API\",\n    description=\"API for order book sequence classification\",\n    version=\"1.0.0\"\n)\n\n# Path to model and pipeline files\nMODEL_PATH = os.getenv(\"MODEL_PATH\", \"order_book_model.pkl\")\nPIPELINE_PATH = os.getenv(\"PIPELINE_PATH\", \"order_book_pipeline.pkl\")\n\n# Load model and pipeline at startup\nmodel = None\npipeline = None\n\nclass ModelLoader:\n    def __init__(self):\n        self._model = None\n        self._pipeline = None\n        self._load_time = None\n\n    def load_model(self):\n        \"\"\"Load model and pipeline from files\"\"\"\n        try:\n            with open(MODEL_PATH, 'rb') as f:\n                self._model = pickle.load(f)\n\n            with open(PIPELINE_PATH, 'rb') as f:\n                self._pipeline = pickle.load(f)\n\n            self._load_time = datetime.now()\n            logger.info(f\"Model loaded successfully at {self._load_time}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Error loading model: {str(e)}\")\n            return False\n\n    @property\n    def model(self):\n        if not self._model:\n            self.load_model()\n        return self._model\n\n    @property\n    def pipeline(self):\n        if not self._pipeline:\n            self.load_model()\n        return self._pipeline\n\n    @property\n    def load_time(self):\n        return self._load_time\n\n# Initialize model loader\nmodel_loader = ModelLoader()\n\n# Pydantic models for request/response\nclass OrderBookEvent(BaseModel):\n    venue: str\n    action: str\n    trade: bool\n    bid: float\n    ask: float\n    price: float\n    bid_size: float\n    ask_size: float\n    flux: float\n\nclass OrderBookSequence(BaseModel):\n    events: List[OrderBookEvent] = Field(..., min_items=100, max_items=100,\n                                        description=\"Sequence of 100 order book events\")\n\nclass PredictionResponse(BaseModel):\n    prediction: int\n    prediction_probability: float\n    processing_time_ms: float\n    timestamp: str\n\nclass StatusResponse(BaseModel):\n    status: str\n    uptime: str\n    model_loaded: bool\n    model_load_time: Optional[str] = None\n\nclass HealthResponse(BaseModel):\n    status: int\n    message: str\n\n# Dependency to ensure model is loaded\ndef get_model():\n    if model_loader.model is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n    return model_loader.model\n\ndef get_pipeline():\n    if model_loader.pipeline is None:\n        raise HTTPException(status_code=503, detail=\"Pipeline not loaded\")\n    return model_loader.pipeline\n\napp_start_time = datetime.now()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model on startup\"\"\"\n    success = model_loader.load_model()\n    if not success:\n        logger.warning(\"Failed to load model at startup. Will attempt to load on first request.\")\n\n@app.get(\"/\", response_model=StatusResponse)\ndef read_root():\n    \"\"\"Get API status\"\"\"\n    # At the beginning of your code\n    current_file_path = '/content/project.ipynb'\n\n    # Then in your read_root function\n    start_time = app_start_time\n    uptime = str(datetime.now() - start_time)\n\n    response = {\n        \"status\": \"running\",\n        \"uptime\": uptime,\n        \"model_loaded\": model_loader.model is not None\n    }\n\n    if model_loader.load_time:\n        response[\"model_load_time\"] = model_loader.load_time.isoformat()\n\n    return response\n\n@app.get(\"/health\", response_model=HealthResponse)\ndef health_check():\n    \"\"\"Check API health\"\"\"\n    if model_loader.model is None:\n        return HealthResponse(status=503, message=\"Model not loaded\")\n    return HealthResponse(status=200, message=\"OK\")\n\n@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict(sequence: OrderBookSequence,\n            model=Depends(get_model),\n            pipeline=Depends(get_pipeline)):\n    \"\"\"Make prediction for a new observation\"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Convert pydantic model to DataFrame\n        df = pd.DataFrame([event.dict() for event in sequence.events])\n\n        # Add observation ID column required by the pipeline\n        df['obs_id'] = 'new_observation'\n\n        # Process using pipeline\n        reshaper = pipeline.named_steps['reshaper']\n        vectorizer = pipeline.named_steps['vectorizer']\n\n        # Transform sequence\n        sequence_dict = reshaper.transform({'new_observation': df})\n        X_tensor, _ = vectorizer.transform(sequence_dict)\n\n        # Make prediction\n        probabilities = model.model.predict(X_tensor)[0]\n        prediction = np.argmax(probabilities)\n        prediction_prob = float(probabilities[prediction])\n\n        # Calculate processing time\n        processing_time = (datetime.now() - start_time).total_seconds() * 1000\n\n        return {\n            \"prediction\": int(prediction),\n            \"prediction_probability\": prediction_prob,\n            \"processing_time_ms\": processing_time,\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n    except Exception as e:\n        logger.error(f\"Error making prediction: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Prediction error: {str(e)}\")\n\n@app.post(\"/reload-model\")\ndef reload_model():\n    \"\"\"Force reload of the model\"\"\"\n    success = model_loader.load_model()\n    if not success:\n        raise HTTPException(status_code=500, detail=\"Failed to reload model\")\n    return {\"message\": \"Model reloaded successfully\", \"load_time\": model_loader.load_time.isoformat()}","metadata":{"id":"8bQFlEyGbfAv","outputId":"1e0240a8-56f9-45de-88d6-074412485c7f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:asyncio:Task exception was never retrieved\n","future: <Task finished name='Task-18' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n","    server.run()\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n","    return asyncio.run(self.serve(sockets=sockets))\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n","    return loop.run_until_complete(task)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n","    self._run_once()\n","  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n","    handle._run()\n","  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n","    self._context.run(self._callback, *self._args)\n","  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n","    self.__step()\n","  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n","    result = coro.send(None)\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n","    with self.capture_signals():\n","  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n","    next(self.gen)\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n","    signal.raise_signal(captured_signal)\n","KeyboardInterrupt\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.8)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n","Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-12-4bbf38bac5ac>:118: DeprecationWarning: \n","        on_event is deprecated, use lifespan event handlers instead.\n","\n","        Read more about it in the\n","        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n","        \n","  @app.on_event(\"startup\")\n"]}],"execution_count":12},{"cell_type":"code","source":"from fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Union, Optional\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport os\nfrom datetime import datetime\nimport logging\nimport nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\nfrom google.colab import userdata\n\n# Load the environment variable\nngrok_auth_token = userdata.get('NGROK_AUTH')\n\n# Check if the token is loaded\nif ngrok_auth_token:\n    print(\"Ngrok Auth Token loaded successfully\")\nelse:\n    print(\"Ngrok Auth Token not found\")\n\n# Configure ngrok with the auth token\nngrok.set_auth_token(ngrok_auth_token)\n\n# Your existing FastAPI application code here\n\ndef run_api():\n    # Apply the nest_asyncio patch\n    nest_asyncio.apply()\n\n    # Create a tunnel to the localhost\n    public_url = ngrok.connect(8000)\n    print(f\"Public URL: {public_url}\")\n\n    # Run the FastAPI application\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n# Launch the API\nrun_api()","metadata":{"id":"VGdiXqKnibEu","outputId":"fd385842-20a6-4b8a-febb-a492a782ae62","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Ngrok Auth Token loaded successfully\n","Public URL: NgrokTunnel: \"https://f53f-34-16-205-53.ngrok-free.app\" -> \"http://localhost:8000\"\n"]},{"output_type":"stream","name":"stderr","text":["INFO:     Started server process [19633]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","WARNING:pyngrok.process.ngrok:t=2025-02-25T10:38:09+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"]},{"output_type":"stream","name":"stdout","text":["INFO:     2a02:8424:61e0:4e01:e566:84a7:6d49:8acb:0 - \"GET / HTTP/1.1\" 200 OK\n","INFO:     2a02:8424:61e0:4e01:e566:84a7:6d49:8acb:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"]}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"jMTzT7b5pmFv"},"outputs":[],"execution_count":null}]}