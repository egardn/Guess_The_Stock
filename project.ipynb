{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "GOOGLE_DRIVE_PATH = '/content/drive/MyDrive/CFM_Guess_The_Stock'\n",
        "X_TRAIN_PICKLE_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'X_train.pkl')\n",
        "Y_TRAIN_PICKLE_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'y_train.pkl')"
      ],
      "metadata": {
        "id": "BSBQgGvYJ7G0",
        "outputId": "f3847469-908f-4a27-df32-1cbeeab756a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s2T_qbsnHQ_w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, GRU, Concatenate, Flatten, Reshape, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# ----------------------------------\n",
        "# 1️⃣ Load & Cache Data Efficiently\n",
        "# ----------------------------------\n",
        "\n",
        "if os.path.exists(X_TRAIN_PICKLE_PATH) and os.path.exists(Y_TRAIN_PICKLE_PATH):\n",
        "    X_train = pd.read_pickle(X_TRAIN_PICKLE_PATH)\n",
        "    y_train = pd.read_pickle(Y_TRAIN_PICKLE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6rUiRg2nHQ_0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Constants for the embedding dimensions\n",
        "VENUE_EMBED_DIM = 8\n",
        "ACTION_EMBED_DIM = 8\n",
        "TRADE_EMBED_DIM = 8\n",
        "SEQ_LENGTH = 100  # Define sequence length as a global constant\n",
        "\n",
        "class OrderBookSequenceReshaper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Reshapes order book data into sequences of observations by obs_id.\n",
        "    Input: DataFrame with order book events\n",
        "    Output: Dictionary of obs_id -> sequence of 100 events\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_length=SEQ_LENGTH):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Group data by observation ID\n",
        "        grouped = X.groupby('obs_id')\n",
        "        sequences = {}\n",
        "\n",
        "        for obs_id, group in grouped:\n",
        "            # Ensure we have exactly seq_length observations\n",
        "            if len(group) == self.seq_length:\n",
        "                sequences[obs_id] = group.reset_index(drop=True)\n",
        "            else:\n",
        "                print(f\"Warning: Observation {obs_id} has {len(group)} events, expected {self.seq_length}\")\n",
        "\n",
        "        return sequences\n",
        "\n",
        "class FeatureVectorizer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Transforms sequences of order book events into tensors of shape (100, 30)\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_length=SEQ_LENGTH):\n",
        "        self.venue_mapping = None\n",
        "        self.action_mapping = None\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def fit(self, sequences, y=None):\n",
        "        # Extract unique values for categorical features\n",
        "        all_venues = set()\n",
        "        all_actions = set()\n",
        "\n",
        "        for seq in sequences.values():\n",
        "            all_venues.update(seq['venue'].unique())\n",
        "            all_actions.update(seq['action'].unique())\n",
        "\n",
        "        # Create mappings for categorical features\n",
        "        self.venue_mapping = {v: i for i, v in enumerate(sorted(all_venues))}\n",
        "        self.action_mapping = {a: i for i, a in enumerate(sorted(all_actions))}\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, sequences):\n",
        "        result = []\n",
        "        obs_ids = []\n",
        "\n",
        "        for obs_id, seq in sequences.items():\n",
        "            # Get first bid price to normalize prices\n",
        "            first_bid = seq.iloc[0]['bid']\n",
        "\n",
        "            # Initialize tensor for this sequence\n",
        "            seq_tensor = np.zeros((self.seq_length, 30))\n",
        "\n",
        "            for i, row in seq.iterrows():\n",
        "                # 1-8: Venue embedding (one-hot for now, will be replaced by embedding)\n",
        "                venue_idx = self.venue_mapping[row['venue']]\n",
        "                seq_tensor[i, venue_idx % 8] = 1  # Use modulo to fit within 8 dimensions\n",
        "\n",
        "                # 9-16: Action embedding\n",
        "                action_idx = self.action_mapping[row['action']]\n",
        "                seq_tensor[i, 8 + (action_idx % 8)] = 1  # Use modulo to fit within 8 dimensions\n",
        "\n",
        "                # 17-24: Trade embedding (one-hot for boolean)\n",
        "                trade_val = 1 if row['trade'] else 0\n",
        "                seq_tensor[i, 16 + trade_val] = 1\n",
        "\n",
        "                # 25: Normalized bid\n",
        "                seq_tensor[i, 24] = row['bid'] - first_bid\n",
        "\n",
        "                # 26: Normalized ask\n",
        "                seq_tensor[i, 25] = row['ask'] - first_bid\n",
        "\n",
        "                # 27: Normalized price\n",
        "                seq_tensor[i, 26] = row['price'] - first_bid\n",
        "\n",
        "                # 28: log(bid_size + 1)\n",
        "                seq_tensor[i, 27] = np.log1p(row['bid_size'])\n",
        "\n",
        "                # 29: log(ask_size + 1)\n",
        "                seq_tensor[i, 28] = np.log1p(row['ask_size'])\n",
        "\n",
        "                # 30: log(flux)\n",
        "                # Handle flux which could be negative\n",
        "                flux = row['flux']\n",
        "                seq_tensor[i, 29] = np.sign(flux) * np.log1p(abs(flux))\n",
        "\n",
        "            result.append(seq_tensor)\n",
        "            obs_ids.append(obs_id)\n",
        "\n",
        "        return np.array(result), obs_ids\n",
        "\n",
        "class OrderBookEmbeddingModel(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Neural network model for order book classification with proper embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, n_venues, n_actions, n_categories=24, batch_size=1000, learning_rate=3e-3, n_batches=10000):\n",
        "        self.n_venues = n_venues\n",
        "        self.n_actions = n_actions\n",
        "        self.n_categories = n_categories\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_batches = n_batches\n",
        "        self.model = None\n",
        "\n",
        "    def create_model(self):\n",
        "        # Input: (100, 30)\n",
        "        input_layer = Input(shape=(SEQ_LENGTH, 30))\n",
        "\n",
        "        # GRU layers - forward and backward\n",
        "        forward_gru = GRU(64, return_sequences=False)(input_layer)\n",
        "        backward_gru = GRU(64, return_sequences=False, go_backwards=True)(input_layer)\n",
        "\n",
        "        # Concatenate GRU outputs\n",
        "        concat = Concatenate()([forward_gru, backward_gru])\n",
        "\n",
        "        # Dense layers\n",
        "        dense1 = Dense(64, activation='selu')(concat)\n",
        "        output_layer = Dense(self.n_categories, activation='softmax')(dense1)\n",
        "\n",
        "        # Create and compile model\n",
        "        model = Model(inputs=input_layer, outputs=output_layer)\n",
        "        model.compile(\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            optimizer=Adam(learning_rate=self.learning_rate),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # X should be the tensor array of shape (n_samples, 100, 30)\n",
        "        # y should be the category labels\n",
        "\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        # Print model summary\n",
        "        self.model.summary()\n",
        "\n",
        "        # Training loop for specified number of batches\n",
        "        for batch in range(self.n_batches):\n",
        "            # Randomly select batch_size samples\n",
        "            if len(X) <= self.batch_size:\n",
        "                X_batch = X\n",
        "                y_batch = y\n",
        "            else:\n",
        "                indices = random.sample(range(len(X)), self.batch_size)\n",
        "                X_batch = X[indices]\n",
        "                y_batch = y[indices]\n",
        "\n",
        "            # Train on batch\n",
        "            loss, acc = self.model.train_on_batch(X_batch, y_batch)\n",
        "\n",
        "            # Report progress every 10 batches\n",
        "            if batch % 10 == 0:\n",
        "                print(f\"Batch {batch}/{self.n_batches} completed - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Return predictions\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Return class predictions\n",
        "        return np.argmax(self.model.predict(X), axis=1)\n",
        "\n",
        "# Define the full pipeline\n",
        "def create_order_book_pipeline():\n",
        "    pipeline = Pipeline([\n",
        "        ('reshaper', OrderBookSequenceReshaper()),\n",
        "        ('vectorizer', FeatureVectorizer()),\n",
        "        # The model would be added after preprocessing in the training script\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "# Training script with subset selection\n",
        "def train_order_book_model(X_train, y_train, n_observations=200, n_batches=100):\n",
        "    \"\"\"\n",
        "    Train the model on a subset of data\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: DataFrame with order book events\n",
        "    - y_train: DataFrame with labels\n",
        "    - n_observations: Number of observations to use for training\n",
        "    - n_batches: Number of batches to train for\n",
        "    \"\"\"\n",
        "    print(f\"Selecting {n_observations} random observations for training...\")\n",
        "\n",
        "    # Get unique observation IDs\n",
        "    obs_ids = X_train['obs_id'].unique()\n",
        "\n",
        "    # Select a random subset of observation IDs\n",
        "    if len(obs_ids) > n_observations:\n",
        "        selected_obs_ids = np.random.choice(obs_ids, size=n_observations, replace=False)\n",
        "    else:\n",
        "        selected_obs_ids = obs_ids\n",
        "        print(f\"Warning: Only {len(obs_ids)} observations available\")\n",
        "\n",
        "    # Filter data to only include selected observations\n",
        "    X_train_subset = X_train[X_train['obs_id'].isin(selected_obs_ids)]\n",
        "    y_train_subset = y_train[y_train.iloc[:, 0].isin(selected_obs_ids)]\n",
        "\n",
        "    print(f\"Selected data shape: {X_train_subset.shape}\")\n",
        "\n",
        "    # Step 1: Data preprocessing\n",
        "    pipeline = create_order_book_pipeline()\n",
        "    sequences = pipeline.named_steps['reshaper'].fit_transform(X_train_subset)\n",
        "    print(f\"Generated {len(sequences)} valid sequences\")\n",
        "\n",
        "    X_tensors, obs_ids = pipeline.named_steps['vectorizer'].fit_transform(sequences)\n",
        "    print(f\"Tensor shape: {X_tensors.shape}\")\n",
        "\n",
        "    # Step 2: Map observation IDs to labels\n",
        "    y_dict = dict(zip(y_train.iloc[:, 0], y_train.iloc[:, 1]))\n",
        "    y_values = np.array([y_dict[obs_id] for obs_id in obs_ids])\n",
        "    print(f\"Label shape: {y_values.shape}\")\n",
        "\n",
        "    # Step 3: Create and train the model\n",
        "    n_venues = len(pipeline.named_steps['vectorizer'].venue_mapping)\n",
        "    n_actions = len(pipeline.named_steps['vectorizer'].action_mapping)\n",
        "\n",
        "    print(f\"Unique venues: {n_venues}, Unique actions: {n_actions}\")\n",
        "\n",
        "    model = OrderBookEmbeddingModel(\n",
        "        n_venues=n_venues,\n",
        "        n_actions=n_actions,\n",
        "        n_batches=n_batches\n",
        "    )\n",
        "\n",
        "    print(f\"Starting training for {n_batches} batches...\")\n",
        "    model.fit(X_tensors, y_values)\n",
        "\n",
        "    return model, pipeline\n",
        "\n",
        "# Main function with small subset training\n",
        "def main(n_observations=500, n_batches=100, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Run the pipeline on a small subset of data with proper train/val splitting\n",
        "    \"\"\"\n",
        "    print(\"Starting training pipeline with small subset...\")\n",
        "\n",
        "    # Get all unique observation IDs first\n",
        "    all_obs_ids = X_train['obs_id'].unique()\n",
        "\n",
        "    # Limit to n_observations if specified\n",
        "    if len(all_obs_ids) > n_observations:\n",
        "        all_selected_ids = np.random.choice(all_obs_ids, size=n_observations, replace=False)\n",
        "    else:\n",
        "        all_selected_ids = all_obs_ids\n",
        "\n",
        "    # Split IDs into train and validation BEFORE any processing\n",
        "    train_ids, val_ids = train_test_split(all_selected_ids, test_size=test_size, random_state=42)\n",
        "\n",
        "    print(f\"Selected {len(train_ids)} observations for training and {len(val_ids)} for validation\")\n",
        "\n",
        "    # Filter data for training\n",
        "    X_train_subset = X_train[X_train['obs_id'].isin(train_ids)]\n",
        "    y_train_subset = y_train[y_train.iloc[:, 0].isin(train_ids)]\n",
        "\n",
        "    # Create pipeline for preprocessing\n",
        "    pipeline = create_order_book_pipeline()\n",
        "\n",
        "    # Process training data\n",
        "    train_sequences = pipeline.named_steps['reshaper'].fit_transform(X_train_subset)\n",
        "    print(f\"Generated {len(train_sequences)} valid sequences for training from {len(train_ids)} observations\")\n",
        "\n",
        "    # If no valid sequences, return early\n",
        "    if len(train_sequences) == 0:\n",
        "        print(\"No valid training sequences found. Check your data.\")\n",
        "        return None, pipeline\n",
        "\n",
        "    X_train_tensors, train_obs_ids = pipeline.named_steps['vectorizer'].fit_transform(train_sequences)\n",
        "\n",
        "    # Map observation IDs to labels\n",
        "    y_dict = dict(zip(y_train.iloc[:, 0], y_train.iloc[:, 1]))\n",
        "    y_train_values = np.array([y_dict[obs_id] for obs_id in train_obs_ids])\n",
        "\n",
        "    # Create and train model\n",
        "    n_venues = len(pipeline.named_steps['vectorizer'].venue_mapping)\n",
        "    n_actions = len(pipeline.named_steps['vectorizer'].action_mapping)\n",
        "\n",
        "    print(f\"Unique venues: {n_venues}, Unique actions: {n_actions}\")\n",
        "    print(f\"Training tensor shape: {X_train_tensors.shape}\")\n",
        "    print(f\"Training labels shape: {y_train_values.shape}\")\n",
        "\n",
        "    model = OrderBookEmbeddingModel(\n",
        "        n_venues=n_venues,\n",
        "        n_actions=n_actions,\n",
        "        n_batches=n_batches\n",
        "    )\n",
        "\n",
        "    print(f\"Starting training for {n_batches} batches...\")\n",
        "    model.fit(X_train_tensors, y_train_values)\n",
        "\n",
        "    # Process validation data if available\n",
        "    if len(val_ids) > 0:\n",
        "        print(f\"\\nValidating on {len(val_ids)} observations...\")\n",
        "\n",
        "        X_val_subset = X_train[X_train['obs_id'].isin(val_ids)]\n",
        "\n",
        "        # Process validation data independently\n",
        "        val_sequences = pipeline.named_steps['reshaper'].transform(X_val_subset)\n",
        "        print(f\"Generated {len(val_sequences)} valid sequences for validation from {len(val_ids)} observations\")\n",
        "\n",
        "        if len(val_sequences) == 0:\n",
        "            print(\"No valid validation sequences found. Skipping validation.\")\n",
        "            return model, pipeline\n",
        "\n",
        "        X_val_tensors, val_obs_ids = pipeline.named_steps['vectorizer'].transform(val_sequences)\n",
        "\n",
        "        # Map observation IDs to labels\n",
        "        y_val_values = np.array([y_dict[obs_id] for obs_id in val_obs_ids])\n",
        "\n",
        "        # Make predictions\n",
        "        print(f\"Making predictions on {len(val_obs_ids)} validation samples...\")\n",
        "        val_predictions = model.predict(X_val_tensors)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = (val_predictions == y_val_values).mean()\n",
        "        print(f\"Validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return model, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, pipeline = main(n_observations=1000, n_batches=100)\n",
        "\n",
        "if model and pipeline:\n",
        "        with open('order_book_model.pkl', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        with open('order_book_pipeline.pkl', 'wb') as f:\n",
        "            pickle.dump(pipeline, f)"
      ],
      "metadata": {
        "id": "EPSFSgC2Vy-J",
        "outputId": "e7202c4b-2dad-4127-97c8-80014f26f46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training pipeline with small subset...\n",
            "Selected 800 observations for training and 200 for validation\n",
            "Generated 800 valid sequences for training from 800 observations\n",
            "Unique venues: 6, Unique actions: 3\n",
            "Training tensor shape: (800, 100, 30)\n",
            "Training labels shape: (800,)\n",
            "Starting training for 100 batches...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m18,432\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m18,432\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │          \u001b[38;5;34m1,560\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,432</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,432</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,560</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,680\u001b[0m (182.34 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,680</span> (182.34 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,680\u001b[0m (182.34 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,680</span> (182.34 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/100 completed - Loss: 3.6470, Accuracy: 0.0350\n",
            "Batch 10/100 completed - Loss: 3.1907, Accuracy: 0.0835\n",
            "Batch 20/100 completed - Loss: 3.0405, Accuracy: 0.1181\n",
            "Batch 30/100 completed - Loss: 2.9047, Accuracy: 0.1475\n",
            "Batch 40/100 completed - Loss: 2.7756, Accuracy: 0.1786\n",
            "Batch 50/100 completed - Loss: 2.6547, Accuracy: 0.2098\n",
            "Batch 60/100 completed - Loss: 2.5380, Accuracy: 0.2421\n",
            "Batch 70/100 completed - Loss: 2.4237, Accuracy: 0.2753\n",
            "Batch 80/100 completed - Loss: 2.3102, Accuracy: 0.3084\n",
            "Batch 90/100 completed - Loss: 2.1951, Accuracy: 0.3437\n",
            "\n",
            "Validating on 200 observations...\n",
            "Generated 200 valid sequences for validation from 200 observations\n",
            "Making predictions on 200 validation samples...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Validation accuracy: 0.1550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok\n",
        "\n",
        "from fastapi import FastAPI, HTTPException, Depends\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Union, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# FastAPI application\n",
        "app = FastAPI(\n",
        "    title=\"Order Book Prediction API\",\n",
        "    description=\"API for order book sequence classification\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Path to model and pipeline files\n",
        "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"order_book_model.pkl\")\n",
        "PIPELINE_PATH = os.getenv(\"PIPELINE_PATH\", \"order_book_pipeline.pkl\")\n",
        "\n",
        "# Load model and pipeline at startup\n",
        "model = None\n",
        "pipeline = None\n",
        "\n",
        "class ModelLoader:\n",
        "    def __init__(self):\n",
        "        self._model = None\n",
        "        self._pipeline = None\n",
        "        self._load_time = None\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load model and pipeline from files\"\"\"\n",
        "        try:\n",
        "            with open(MODEL_PATH, 'rb') as f:\n",
        "                self._model = pickle.load(f)\n",
        "\n",
        "            with open(PIPELINE_PATH, 'rb') as f:\n",
        "                self._pipeline = pickle.load(f)\n",
        "\n",
        "            self._load_time = datetime.now()\n",
        "            logger.info(f\"Model loaded successfully at {self._load_time}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        if not self._model:\n",
        "            self.load_model()\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def pipeline(self):\n",
        "        if not self._pipeline:\n",
        "            self.load_model()\n",
        "        return self._pipeline\n",
        "\n",
        "    @property\n",
        "    def load_time(self):\n",
        "        return self._load_time\n",
        "\n",
        "# Initialize model loader\n",
        "model_loader = ModelLoader()\n",
        "\n",
        "# Pydantic models for request/response\n",
        "class OrderBookEvent(BaseModel):\n",
        "    venue: str\n",
        "    action: str\n",
        "    trade: bool\n",
        "    bid: float\n",
        "    ask: float\n",
        "    price: float\n",
        "    bid_size: float\n",
        "    ask_size: float\n",
        "    flux: float\n",
        "\n",
        "class OrderBookSequence(BaseModel):\n",
        "    events: List[OrderBookEvent] = Field(..., min_items=100, max_items=100,\n",
        "                                        description=\"Sequence of 100 order book events\")\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    prediction: int\n",
        "    prediction_probability: float\n",
        "    processing_time_ms: float\n",
        "    timestamp: str\n",
        "\n",
        "class StatusResponse(BaseModel):\n",
        "    status: str\n",
        "    uptime: str\n",
        "    model_loaded: bool\n",
        "    model_load_time: Optional[str] = None\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: int\n",
        "    message: str\n",
        "\n",
        "# Dependency to ensure model is loaded\n",
        "def get_model():\n",
        "    if model_loader.model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "    return model_loader.model\n",
        "\n",
        "def get_pipeline():\n",
        "    if model_loader.pipeline is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Pipeline not loaded\")\n",
        "    return model_loader.pipeline\n",
        "\n",
        "app_start_time = datetime.now()\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    \"\"\"Load model on startup\"\"\"\n",
        "    success = model_loader.load_model()\n",
        "    if not success:\n",
        "        logger.warning(\"Failed to load model at startup. Will attempt to load on first request.\")\n",
        "\n",
        "@app.get(\"/\", response_model=StatusResponse)\n",
        "def read_root():\n",
        "    \"\"\"Get API status\"\"\"\n",
        "    # At the beginning of your code\n",
        "    current_file_path = '/content/project.ipynb'\n",
        "\n",
        "    # Then in your read_root function\n",
        "    start_time = app_start_time\n",
        "    uptime = str(datetime.now() - start_time)\n",
        "\n",
        "    response = {\n",
        "        \"status\": \"running\",\n",
        "        \"uptime\": uptime,\n",
        "        \"model_loaded\": model_loader.model is not None\n",
        "    }\n",
        "\n",
        "    if model_loader.load_time:\n",
        "        response[\"model_load_time\"] = model_loader.load_time.isoformat()\n",
        "\n",
        "    return response\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "def health_check():\n",
        "    \"\"\"Check API health\"\"\"\n",
        "    if model_loader.model is None:\n",
        "        return HealthResponse(status=503, message=\"Model not loaded\")\n",
        "    return HealthResponse(status=200, message=\"OK\")\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "def predict(sequence: OrderBookSequence,\n",
        "            model=Depends(get_model),\n",
        "            pipeline=Depends(get_pipeline)):\n",
        "    \"\"\"Make prediction for a new observation\"\"\"\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    try:\n",
        "        # Convert pydantic model to DataFrame\n",
        "        df = pd.DataFrame([event.dict() for event in sequence.events])\n",
        "\n",
        "        # Add observation ID column required by the pipeline\n",
        "        df['obs_id'] = 'new_observation'\n",
        "\n",
        "        # Process using pipeline\n",
        "        reshaper = pipeline.named_steps['reshaper']\n",
        "        vectorizer = pipeline.named_steps['vectorizer']\n",
        "\n",
        "        # Transform sequence\n",
        "        sequence_dict = reshaper.transform({'new_observation': df})\n",
        "        X_tensor, _ = vectorizer.transform(sequence_dict)\n",
        "\n",
        "        # Make prediction\n",
        "        probabilities = model.model.predict(X_tensor)[0]\n",
        "        prediction = np.argmax(probabilities)\n",
        "        prediction_prob = float(probabilities[prediction])\n",
        "\n",
        "        # Calculate processing time\n",
        "        processing_time = (datetime.now() - start_time).total_seconds() * 1000\n",
        "\n",
        "        return {\n",
        "            \"prediction\": int(prediction),\n",
        "            \"prediction_probability\": prediction_prob,\n",
        "            \"processing_time_ms\": processing_time,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error making prediction: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Prediction error: {str(e)}\")\n",
        "\n",
        "@app.post(\"/reload-model\")\n",
        "def reload_model():\n",
        "    \"\"\"Force reload of the model\"\"\"\n",
        "    success = model_loader.load_model()\n",
        "    if not success:\n",
        "        raise HTTPException(status_code=500, detail=\"Failed to reload model\")\n",
        "    return {\"message\": \"Model reloaded successfully\", \"load_time\": model_loader.load_time.isoformat()}"
      ],
      "metadata": {
        "id": "8bQFlEyGbfAv",
        "outputId": "1e0240a8-56f9-45de-88d6-074412485c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-18' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.8)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-4bbf38bac5ac>:118: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException, Depends\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Union, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the environment variable\n",
        "ngrok_auth_token = userdata.get('NGROK_AUTH')\n",
        "\n",
        "# Check if the token is loaded\n",
        "if ngrok_auth_token:\n",
        "    print(\"Ngrok Auth Token loaded successfully\")\n",
        "else:\n",
        "    print(\"Ngrok Auth Token not found\")\n",
        "\n",
        "# Configure ngrok with the auth token\n",
        "ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "# Your existing FastAPI application code here\n",
        "\n",
        "def run_api():\n",
        "    # Apply the nest_asyncio patch\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Create a tunnel to the localhost\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "\n",
        "    # Run the FastAPI application\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Launch the API\n",
        "run_api()"
      ],
      "metadata": {
        "id": "VGdiXqKnibEu",
        "outputId": "fd385842-20a6-4b8a-febb-a492a782ae62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok Auth Token loaded successfully\n",
            "Public URL: NgrokTunnel: \"https://f53f-34-16-205-53.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [19633]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "WARNING:pyngrok.process.ngrok:t=2025-02-25T10:38:09+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2a02:8424:61e0:4e01:e566:84a7:6d49:8acb:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2a02:8424:61e0:4e01:e566:84a7:6d49:8acb:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMTzT7b5pmFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}